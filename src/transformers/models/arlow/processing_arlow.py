#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/arlow/modular_arlow.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_arlow.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
from typing import Optional, Union

import numpy as np
import torch

from ...feature_extraction_utils import BatchFeature
from ...image_utils import ImageInput
from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack
from ...tokenization_utils_base import PreTokenizedInput, TextInput
from ...utils import logging
from ...video_utils import VideoInput


logger = logging.get_logger(__name__)


class ArlowProcessorKwargs(ProcessingKwargs, total=False):
    _defaults = {
        "images_kwargs": {},
        "text_kwargs": {
            "padding": False,
            "return_token_type_ids": False,
            "return_mm_token_type_ids": False,
        },
        "videos_kwargs": {"return_metadata": True},
    }


class ArlowProcessor(ProcessorMixin):
    r"""
    Constructs an Arlow processor which wraps an image processor, a tokenizer, and a video processor into a single
    processor. Follows Qwen3VL's strategy for placeholder expansion and timestamp prompts.

    Args:
        image_processor: Required image processor.
        tokenizer: Required tokenizer (`ArlowTokenizer` or `ArlowTokenizerFast`).
        video_processor: Required video processor for video support.
        chat_template: Optional chat template string.
    """

    attributes = ["image_processor", "tokenizer", "video_processor"]
    image_processor_class = "AutoImageProcessor"
    video_processor_class = "AutoVideoProcessor"
    tokenizer_class = ("ArlowTokenizer", "ArlowTokenizerFast")

    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):
        # multimodal special tokens
        self.image_token = "<image>" if not hasattr(tokenizer, "image_token") else tokenizer.image_token
        self.video_token = "<video>" if not hasattr(tokenizer, "video_token") else tokenizer.video_token
        self.image_token_id = (
            tokenizer.image_token_id
            if getattr(tokenizer, "image_token_id", None)
            else tokenizer.convert_tokens_to_ids(self.image_token)
        )
        self.video_token_id = (
            tokenizer.video_token_id
            if getattr(tokenizer, "video_token_id", None)
            else tokenizer.convert_tokens_to_ids(self.video_token)
        )

        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)

        self.vision_start_token = (
            "<|vision_start|>" if not hasattr(tokenizer, "vision_start_token") else tokenizer.vision_start_token
        )
        self.vision_end_token = (
            "<|vision_end|>" if not hasattr(tokenizer, "vision_end_token") else tokenizer.vision_end_token
        )
        self.vision_start_token_id = (
            tokenizer.vision_start_token_id
            if getattr(tokenizer, "vision_start_token_id", None)
            else tokenizer.convert_tokens_to_ids(self.vision_start_token)
        )
        self.vision_end_token_id = (
            tokenizer.vision_end_token_id
            if getattr(tokenizer, "vision_end_token_id", None)
            else tokenizer.convert_tokens_to_ids(self.vision_end_token)
        )

    def __call__(
        self,
        images: ImageInput | None = None,
        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] | None = None,
        videos: VideoInput | None = None,
        **kwargs: Unpack[ArlowProcessorKwargs],
    ) -> BatchFeature:
        output_kwargs = self._merge_kwargs(
            ArlowProcessorKwargs,
            tokenizer_init_kwargs=self.tokenizer.init_kwargs,
            **kwargs,
        )

        image_inputs = {}
        video_inputs = {}
        image_grid_thw = None
        video_grid_thw = None
        image_num_crops: Optional[list[int]] = None

        if images is not None:
            image_inputs = self.image_processor(images=images, **output_kwargs["images_kwargs"])
            image_grid_thw = image_inputs.get("image_grid_thw")
            raw_num_crops = image_inputs.pop("num_crops", None)
            if raw_num_crops is not None:
                if isinstance(raw_num_crops, torch.Tensor):
                    image_num_crops = raw_num_crops.tolist()
                else:
                    image_num_crops = list(raw_num_crops)

        if videos is not None:
            video_inputs = self.video_processor(videos=videos, **output_kwargs["videos_kwargs"])
            video_grid_thw = video_inputs.get("video_grid_thw")
            # preserve metadata for timestamp prompts if provided
            if "return_metadata" not in kwargs and "video_metadata" in video_inputs:
                video_metadata = video_inputs.pop("video_metadata")
            else:
                video_metadata = video_inputs.get("video_metadata")

        if not isinstance(text, list):
            text = [text]

        text = text.copy()  # will edit in-place

        # Inject additional image placeholders for pan-and-scan crops
        if image_num_crops is not None:
            placeholder_idx = 0
            for i, prompt in enumerate(text):
                if prompt is None:
                    continue
                occurrences: list[int] = []
                start = 0
                while True:
                    found = prompt.find(self.image_token, start)
                    if found == -1:
                        break
                    occurrences.append(found)
                    start = found + len(self.image_token)

                for pos in reversed(occurrences):
                    if placeholder_idx >= len(image_num_crops):
                        raise ValueError("Mismatch between number of image placeholders in text and image inputs.")
                    crops = image_num_crops[placeholder_idx]
                    if crops > 0:
                        formatted = (
                            f"Here is the original image {self.image_token} and here are some crops to help you see better "
                            + " ".join([self.image_token] * crops)
                        )
                        prompt = prompt[:pos] + formatted + prompt[pos + len(self.image_token) :]
                    placeholder_idx += 1
                text[i] = prompt

            if placeholder_idx != len(image_num_crops):
                raise ValueError(
                    "Mismatch between number of processed images and placeholders after pan-and-scan expansion."
                )

        # Expand image placeholders by computed token budget
        if image_grid_thw is not None:
            merge_len = self.image_processor.merge_size**2 if hasattr(self.image_processor, "merge_size") else 1
            index = 0
            placeholder_idx = 0
            for i in range(len(text)):
                while text[i] is not None and self.image_token in text[i]:
                    views = 1
                    if image_num_crops is not None:
                        if placeholder_idx >= len(image_num_crops):
                            raise ValueError(
                                "Mismatch between number of image placeholders and pan-and-scan metadata."
                            )
                        views += image_num_crops[placeholder_idx]
                    placeholder_tokens = ""
                    for _ in range(views):
                        if index >= len(image_grid_thw):
                            raise ValueError(
                                "Not enough image grid metadata to expand placeholders. "
                                "Check pan-and-scan configuration."
                            )
                        grid_entry = image_grid_thw[index]
                        if isinstance(grid_entry, torch.Tensor):
                            num_image_tokens = int(torch.prod(grid_entry).item()) // merge_len
                        else:
                            num_image_tokens = int(np.prod(grid_entry)) // merge_len
                        placeholder_tokens += "<|placeholder|>" * num_image_tokens
                        index += 1
                    text[i] = text[i].replace(self.image_token, placeholder_tokens, 1)
                    placeholder_idx += 1
                if text[i] is not None:
                    text[i] = text[i].replace("<|placeholder|>", self.image_token)

            if image_num_crops is not None and placeholder_idx != len(image_num_crops):
                raise ValueError(
                    "Mismatch between number of placeholders processed and pan-and-scan metadata entries."
                )
            if index != len(image_grid_thw):
                logger.warning(
                    "Some image grid metadata entries were unused during placeholder expansion. "
                    "This may indicate mismatched prompts or preprocessing configuration."
                )

        # Expand video placeholders into per-frame segments with optional timestamps
        if video_grid_thw is not None:
            merge_len = self.video_processor.merge_size**2 if hasattr(self.video_processor, "merge_size") else 1
            index = 0
            for i in range(len(text)):
                while self.video_token in text[i]:
                    # build per-frame blocks
                    video_placeholder = ""
                    frame_seqlen = video_grid_thw[index][1:].prod() // merge_len

                    # compute timestamps if metadata exists, otherwise just omit
                    curr_timestamps = None
                    if video_metadata is not None:
                        metadata = video_metadata[index]
                        if getattr(metadata, "fps", None) is None:
                            logger.warning_once(
                                "Arlow requires video fps to build timestamp prompts; defaulting to fps=24."
                            )
                            metadata.fps = 24 if metadata.fps is None else metadata.fps
                        curr_timestamps = self._calculate_timestamps(
                            metadata.frames_indices, metadata.fps, self.video_processor.merge_size
                        )

                    for frame_idx in range(video_grid_thw[index][0]):
                        if curr_timestamps is not None:
                            curr_time = curr_timestamps[frame_idx]
                            video_placeholder += f"<{curr_time:.1f} seconds>"
                        video_placeholder += (
                            self.vision_start_token + "<|placeholder|>" * frame_seqlen + self.vision_end_token
                        )

                    compound = f"{self.vision_start_token}{self.video_token}{self.vision_end_token}"
                    if compound in text[i]:
                        text[i] = text[i].replace(compound, video_placeholder, 1)
                    else:
                        text[i] = text[i].replace(self.video_token, video_placeholder, 1)
                    index += 1

                text[i] = text[i].replace("<|placeholder|>", self.video_token)

        return_tensors = output_kwargs["text_kwargs"].pop("return_tensors", None)
        return_mm_token_type_ids = output_kwargs["text_kwargs"].pop("return_mm_token_type_ids", None)
        text_inputs = self.tokenizer(text, **output_kwargs["text_kwargs"])
        self._check_special_mm_tokens(text, text_inputs, modalities=["image", "video"])

        if return_mm_token_type_ids:
            array_ids = np.array(text_inputs["input_ids"])  # type: ignore[arg-type]
            mm_token_type_ids = np.zeros_like(text_inputs["input_ids"])  # type: ignore[arg-type]
            mm_token_type_ids[array_ids == self.image_token_id] = 1
            text_inputs["mm_token_type_ids"] = mm_token_type_ids.tolist()

        return BatchFeature(data={**text_inputs, **image_inputs, **video_inputs}, tensor_type=return_tensors)

    def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwargs):
        vision_data = {}
        if image_sizes is not None and self.image_processor is not None:
            images_kwargs = ArlowProcessorKwargs._defaults.get("images_kwargs", {}).copy()
            images_kwargs.update(kwargs)
            merge_size = images_kwargs.get("merge_size", None) or getattr(self.image_processor, "merge_size", 1)
            num_image_patches = [
                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)
                for image_size in image_sizes
            ]
            num_image_tokens = [(num_patches // merge_size**2) for num_patches in num_image_patches]
            vision_data.update({"num_image_tokens": num_image_tokens, "num_image_patches": num_image_patches})

        if video_sizes is not None and self.video_processor is not None:
            videos_kwargs = ArlowProcessorKwargs._defaults.get("videos_kwargs", {}).copy()
            videos_kwargs.update(kwargs)
            num_video_patches = [
                self.video_processor.get_number_of_video_patches(*video_size, videos_kwargs)
                for video_size in video_sizes
            ]
            merge_size = getattr(self.video_processor, "merge_size", 1)
            num_video_tokens = [(num_patches // merge_size**2) for num_patches in num_video_patches]
            vision_data["num_video_tokens"] = num_video_tokens

        return MultiModalData(**vision_data)  # type: ignore[name-defined]

    def post_process_image_text_to_text(
        self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs
    ):
        return self.tokenizer.batch_decode(
            generated_outputs,
            skip_special_tokens=skip_special_tokens,
            clean_up_tokenization_spaces=clean_up_tokenization_spaces,
            **kwargs,
        )

    def _calculate_timestamps(self, indices: Union[list[int], np.ndarray], video_fps: float, merge_size: int = 2):
        if not isinstance(indices, list):
            indices = indices.tolist()
        if len(indices) % merge_size != 0:
            indices.extend(indices[-1] for _ in range(merge_size - len(indices) % merge_size))
        timestamps = [idx / video_fps for idx in indices]
        timestamps = [
            (timestamps[i] + timestamps[i + merge_size - 1]) / 2 for i in range(0, len(timestamps), merge_size)
        ]
        return timestamps


__all__ = ["ArlowProcessor"]
