#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/arlow/modular_arlow.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_arlow.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
import math
from typing import Optional

from ...configuration_utils import PreTrainedConfig, layer_type_validation
from ...modeling_rope_utils import rope_config_validation


class ArlowVisionConfig(PreTrainedConfig):
    r"""
    Configuration for the vision transformer component of Arlow multimodal models.

    Args:
        depth (`int`, *optional*, defaults to 32):
            Number of hidden layers in the vision transformer.
        embed_dim (`int`, *optional*, defaults to 1280):
            Dimensionality of the vision encoder embeddings.
        hidden_size (`int`, *optional*, defaults to 3584):
            Dimensionality after vision projection to match text model.
        hidden_act (`str`, *optional*, defaults to `"gelu_pytorch_tanh"`):
            The non-linear activation function in the vision encoder.
        mlp_ratio (`int`, *optional*, defaults to 4):
            Ratio of mlp hidden dim to embedding dim.
        num_heads (`int`, *optional*, defaults to 16):
            Number of attention heads in the vision transformer.
        in_channels (`int`, *optional*, defaults to 3):
            Number of input image channels.
        patch_size (`int`, *optional*, defaults to 14):
            Size of image patches.
        spatial_merge_size (`int`, *optional*, defaults to 2):
            Spatial merge factor for patch merging.
        temporal_patch_size (`int`, *optional*, defaults to 2):
            Temporal patch size for video inputs.
        use_deformable_attention (`bool`, *optional*, defaults to False):
            Whether to apply deformable attention bias for high-resolution regions.
        use_progressive_patches (`bool`, *optional*, defaults to False):
            Whether to inject progressive multi-scale patch features.
        token_pruning_ratio (`float`, *optional*, defaults to 0.0):
            Ratio of tokens to prune per region (0.0 means no pruning).
        deformable_attention_window (`float`, *optional*, defaults to 0.25):
            Normalized radius (in THW coordinates) used to compute deformable attention locality bias.
        deformable_attention_strength (`float`, *optional*, defaults to 4.0):
            Scaling factor applied to deformable attention bias.
        deepstack_visual_indexes (`list[int]`, *optional*):
            Vision block indexes from which to capture DeepStack skip features. Defaults to a
            depth-aware spread when unset (empty when depth < 6).
        mrope_sections (`list[int]`, *optional*):
            Multimodal RoPE allocation across [temporal, height, width] (sum must equal head dimension).
        initializer_range (`float`, *optional*, defaults to 0.02):
            Standard deviation for weight initialization.
    """

    model_type = "arlow"
    base_config_key = "vision_config"

    def __init__(
        self,
        depth: int = 2,
        embed_dim: int = 32,
        hidden_size: int = 64,
        hidden_act: str = "gelu_pytorch_tanh",
        mlp_ratio: int = 4,
        num_heads: int = 4,
        in_channels: int = 3,
        patch_size: int = 14,
        spatial_merge_size: int = 2,
        temporal_patch_size: int = 2,
        use_deformable_attention: bool = False,
        use_progressive_patches: bool = False,
        token_pruning_ratio: float = 0.0,
        deformable_attention_window: float = 0.25,
        deformable_attention_strength: float = 4.0,
        deepstack_visual_indexes: Optional[list[int]] = None,
        mrope_sections: Optional[list[int]] = None,
        initializer_range: float = 0.02,
        max_position_embeddings: Optional[int] = None,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.depth = depth
        self.embed_dim = embed_dim
        self.hidden_size = hidden_size
        self.hidden_act = hidden_act
        self.mlp_ratio = mlp_ratio
        self.num_heads = num_heads
        self.num_attention_heads = num_heads
        self.in_channels = in_channels
        self.patch_size = patch_size
        self.spatial_merge_size = spatial_merge_size
        self.temporal_patch_size = temporal_patch_size
        self.use_deformable_attention = use_deformable_attention
        self.use_progressive_patches = use_progressive_patches
        self.token_pruning_ratio = token_pruning_ratio
        self.deformable_attention_window = deformable_attention_window
        self.deformable_attention_strength = deformable_attention_strength
        self.initializer_range = initializer_range
        self.max_position_embeddings = max_position_embeddings if max_position_embeddings is not None else 32768
        if deepstack_visual_indexes is None:
            if depth >= 6:
                approx = {
                    max(0, depth // 4),
                    max(0, depth // 2),
                    max(0, depth - 4),
                }
                deepstack_visual_indexes = sorted(idx for idx in approx if 0 <= idx < depth)
            else:
                deepstack_visual_indexes = []
        else:
            deepstack_visual_indexes = sorted(set(idx for idx in deepstack_visual_indexes if 0 <= idx < depth))
        self.deepstack_visual_indexes = deepstack_visual_indexes

        head_dim = embed_dim // num_heads
        if mrope_sections is None:
            t = max(1, head_dim // 3)
            h = max(1, (head_dim - t) // 2)
            w = max(1, head_dim - t - h)
            if t + h + w != head_dim:
                w = head_dim - t - h
            self.mrope_sections = [t, h, w]
        else:
            if sum(mrope_sections) != head_dim:
                raise ValueError(
                    f"Sum of mrope_sections {mrope_sections} (={sum(mrope_sections)}) must equal head_dim ({head_dim})."
                )
            self.mrope_sections = mrope_sections


class ArlowTextConfig(PreTrainedConfig):
    r"""
    Text-only configuration for Arlow.

    This configuration is used by text-only models like `ArlowTextModel` and `ArlowForCausalLM`.
    """

    model_type = "arlow_text"
    base_config_key = "text_config"

    def __init__(
        self,
        vocab_size=131072,
        hidden_size=2304,
        intermediate_size=9216,
        num_hidden_layers=32,
        num_attention_heads=24,
        num_key_value_heads=4,
        hidden_act="silu",
        max_position_embeddings=32768,
        initializer_range=0.02,
        rms_norm_eps=1e-6,
        use_cache=True,
        pad_token_id=None,
        bos_token_id=None,
        eos_token_id=None,
        tie_word_embeddings=False,
        rope_theta=100000.0,
        rope_parameters=None,
        rope_scaling=None,  # Deprecated, use rope_parameters
        attention_bias=False,
        attention_dropout=0.0,
        resid_dropout=0.0,
        mlp_dropout=0.0,
        head_dim=None,
        use_sliding_window=False,
        sliding_window=4096,
        max_window_layers=28,
        layer_types=None,
        mrope_sections=None,
        **kwargs,
    ):
        self.vocab_size = vocab_size
        self.max_position_embeddings = max_position_embeddings
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.num_key_value_heads = num_key_value_heads
        self.hidden_act = hidden_act
        self.initializer_range = initializer_range
        self.rms_norm_eps = rms_norm_eps
        self.use_cache = use_cache
        self.rope_theta = rope_theta
        self.rope_parameters = rope_scaling or rope_parameters
        self.attention_bias = attention_bias
        self.attention_dropout = attention_dropout
        self.resid_dropout = resid_dropout
        self.mlp_dropout = mlp_dropout
        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads

        # Provide default M-ROPE sections for text model as well
        if mrope_sections is None:
            # Split head_dim approximately equally across [T, H, W]
            t = max(1, self.head_dim // 3)
            h = max(1, (self.head_dim - t) // 2)
            w = max(1, self.head_dim - t - h)
            # Adjust last to ensure exact sum
            if t + h + w != self.head_dim:
                w = self.head_dim - t - h
            self.mrope_sections = [t, h, w]
        else:
            self.mrope_sections = mrope_sections

        # Validate rope parameters
        if self.rope_parameters is not None and "type" in self.rope_parameters:
            self.rope_parameters["rope_type"] = self.rope_parameters["type"]
        rope_config_validation(self, ignore_keys={"mrope_sections"})

        # Layer types configuration (supports full/sliding attention)
        self.use_sliding_window = use_sliding_window
        self.sliding_window = sliding_window if self.use_sliding_window else None
        self.max_window_layers = max_window_layers
        self.layer_types = layer_types
        if self.layer_types is None:
            self.layer_types = [
                "sliding_attention"
                if self.sliding_window is not None and i >= self.max_window_layers
                else "full_attention"
                for i in range(self.num_hidden_layers)
            ]
        layer_type_validation(self.layer_types, self.num_hidden_layers)

        super().__init__(
            pad_token_id=pad_token_id,
            bos_token_id=bos_token_id,
            eos_token_id=eos_token_id,
            tie_word_embeddings=tie_word_embeddings,
            **kwargs,
        )


class ArlowConfig(PreTrainedConfig):
    r"""
    Configuration class for Arlow multimodal models.

    This is the configuration for the main Arlow vision-language model. Use ArlowTextConfig if you need
    a text-only configuration.

    Instantiating with defaults yields configuration similar to Arlow-Base
    [yuchenxie/ArlowGPT-Base](https://huggingface.co/yuchenxie/ArlowGPT-Base).

    Configuration objects inherit from [`PretrainedConfig`] and control model outputs.

    Args:
        vocab_size (`int`, *optional*, defaults to 131072):
            Vocabulary size of the model.
        hidden_size (`int`, *optional*, defaults to 2304):
            Dimension of hidden representations.
        intermediate_size (`int`, *optional*, defaults to 9216):
            Dimension of MLP representations.
        num_hidden_layers (`int`, *optional*, defaults to 32):
            Number of hidden layers in the Transformer decoder.
        num_attention_heads (`int`, *optional*, defaults to 24):
            Number of attention heads per layer.
        num_key_value_heads (`int`, *optional*, defaults to 4):
            Number of key_value heads for Grouped Query Attention.
        hidden_act (`str`, *optional*, defaults to `"silu"`):
            Non-linear activation function.
        max_position_embeddings (`int`, *optional*, defaults to 2048):
            Maximum sequence length.
        initializer_range (`float`, *optional*, defaults to 0.02):
            Standard deviation for weight initialization.
        rms_norm_eps (`float`, *optional*, defaults to 1e-06):
            Epsilon for RMS normalization layers.
        use_cache (`bool`, *optional*, defaults to `True`):
            Whether to return past key/values for faster decoding.
        pad_token_id (`int`, *optional*):
            Padding token ID.
        bos_token_id (`int`, *optional*):
            Beginning of sequence token ID.
        eos_token_id (`int`, *optional*):
            End of sequence token ID.
        tie_word_embeddings (`bool`, *optional*, defaults to `False`):
            Whether to tie input/output embeddings.
        rope_theta (`float`, *optional*, defaults to 100000.0):
            Base period of RoPE embeddings.
        rope_parameters (`Dict`, *optional*):
            RoPE scaling configuration. For backwards compatibility, `rope_scaling` is also accepted.
        attention_bias (`bool`, *optional*, defaults to `False`):
            Whether to use bias in attention projections.
        attention_dropout (`float`, *optional*, defaults to 0.0):
            Dropout ratio for attention probabilities.
        resid_dropout (`float`, *optional*, defaults to 0.0):
            Dropout ratio for residual connections.
        mlp_dropout (`float`, *optional*, defaults to 0.0):
            Dropout ratio for MLP layers.
        head_dim (`int`, *optional*):
            Attention head dimension. Defaults to hidden_size // num_attention_heads.
        use_sliding_window (`bool`, *optional*, defaults to `False`):
            Whether to use sliding window attention.
        sliding_window (`int`, *optional*, defaults to 4096):
            Sliding window size.
        max_window_layers (`int`, *optional*, defaults to 28):
            Number of layers using full attention before switching to sliding window.
        layer_types (`list`, *optional*):
            Attention pattern for each layer.
        vision_config (`Union[PreTrainedConfig, dict]`, *optional*):
            Vision backbone configuration.
        mm_tokens_per_image (`int`, *optional*, defaults to 256):
            Number of tokens per image after vision projection.
        mm_tokens_per_video (`int`, *optional*, defaults to 128):
            Number of tokens per video after temporal resampling.
        video_max_frames (`int`, *optional*, defaults to 64):
            Maximum number of video frames to extract.
        video_sample_strategy (`str`, *optional*, defaults to "uniform"):
            Video frame sampling strategy: "uniform", "motion_adaptive", or "fps_based".
        dynamic_resolution (`bool`, *optional*, defaults to True):
            Whether to use dynamic resolution for images.
        pan_and_scan (`bool`, *optional*, defaults to False):
            Whether to use pan-and-scan to keep token budgets constant.
        timestamp_alignment (`bool`, *optional*, defaults to False):
            Whether to enable timestamp supervision for video grounding.
        use_gated_cross_attention (`bool`, *optional*, defaults to False):
            Whether to use gated cross-attention in upper layers.
        gated_cross_attention_start_layer (`int`, *optional*):
            Layer index to start gated cross-attention (if enabled).
        image_token_id (`int`, *optional*):
            Token ID for image placeholders.
        video_token_id (`int`, *optional*):
            Token ID for video placeholders.
        vision_start_token_id (`int`, *optional*):
            Token ID marking start of vision input.
        vision_end_token_id (`int`, *optional*):
            Token ID marking end of vision input.
        frame_separator_token_id (`int`, *optional*):
            Token ID for separating video frames.
        mrope_sections (`list`, *optional*):
            M-ROPE sections for [temporal, height, width] dimensions.
            Defaults to split based on head_dim.
    """

    model_type = "arlow"
    sub_configs = {"vision_config": ArlowVisionConfig}
    keys_to_ignore_at_inference = ["past_key_values"]

    def __init__(
        self,
        vocab_size=131072,
        hidden_size=2304,
        intermediate_size=9216,
        num_hidden_layers=32,
        num_attention_heads=24,
        num_key_value_heads=4,
        hidden_act="silu",
        max_position_embeddings=32768,
        initializer_range=0.02,
        rms_norm_eps=1e-6,
        use_cache=True,
        pad_token_id=None,
        bos_token_id=None,
        eos_token_id=None,
        tie_word_embeddings=False,
        rope_theta=100000.0,
        rope_parameters=None,
        rope_scaling=None,  # Deprecated, use rope_parameters
        attention_bias=False,
        attention_dropout=0.0,
        resid_dropout=0.0,
        mlp_dropout=0.0,
        head_dim=None,
        use_sliding_window=False,
        sliding_window=4096,
        max_window_layers=28,
        layer_types=None,
        # Multimodal parameters
        vision_config=None,
        mm_tokens_per_image=512,
        mm_tokens_per_video=1024,
        video_max_frames=768,
        video_sample_strategy="uniform",
        dynamic_resolution=True,
        pan_and_scan=False,
        timestamp_alignment=False,
        use_gated_cross_attention=False,
        gated_cross_attention_start_layer=None,
        image_token_id=None,
        video_token_id=None,
        vision_start_token_id=None,
        vision_end_token_id=None,
        frame_separator_token_id=None,
        mrope_sections=None,
        **kwargs,
    ):
        self.vocab_size = vocab_size
        self.max_position_embeddings = max_position_embeddings
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.use_sliding_window = use_sliding_window
        self.sliding_window = sliding_window if self.use_sliding_window else None
        self.max_window_layers = max_window_layers
        self.num_key_value_heads = num_key_value_heads
        self.hidden_act = hidden_act
        self.initializer_range = initializer_range
        self.rms_norm_eps = rms_norm_eps
        self.use_cache = use_cache
        self.rope_theta = rope_theta
        self.rope_parameters = rope_scaling or rope_parameters
        self.attention_bias = attention_bias
        self.attention_dropout = attention_dropout
        self.resid_dropout = resid_dropout
        self.mlp_dropout = mlp_dropout
        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads

        # Multimodal configuration
        if isinstance(vision_config, dict):
            self.vision_config = ArlowVisionConfig(**vision_config)
        elif vision_config is None:
            self.vision_config = ArlowVisionConfig()
        else:
            self.vision_config = vision_config

        self.mm_tokens_per_image = mm_tokens_per_image
        self.mm_tokens_per_video = mm_tokens_per_video
        self.video_max_frames = video_max_frames
        self.video_sample_strategy = video_sample_strategy
        self.dynamic_resolution = dynamic_resolution
        self.pan_and_scan = pan_and_scan
        self.timestamp_alignment = timestamp_alignment
        self.use_gated_cross_attention = use_gated_cross_attention
        self.gated_cross_attention_start_layer = gated_cross_attention_start_layer
        self.image_token_id = image_token_id
        self.video_token_id = video_token_id
        self.vision_start_token_id = vision_start_token_id
        self.vision_end_token_id = vision_end_token_id
        self.frame_separator_token_id = frame_separator_token_id

        # M-ROPE configuration: default sections based on head_dim
        if mrope_sections is None:
            # Split head_dim into temporal, height, width sections with small-safe defaults
            t = max(1, self.head_dim // 3)
            h = max(1, (self.head_dim - t) // 2)
            w = max(1, self.head_dim - t - h)
            if t + h + w != self.head_dim:
                w = self.head_dim - t - h
            self.mrope_sections = [t, h, w]
        else:
            self.mrope_sections = mrope_sections

        # Validate mrope_sections sum equals head_dim
        if sum(self.mrope_sections) != self.head_dim:
            raise ValueError(
                f"Sum of mrope_sections {self.mrope_sections} (={sum(self.mrope_sections)}) "
                f"must equal head_dim ({self.head_dim})"
            )

        # Keep track of the ratio so we can project it onto the vision head dimension
        self._mrope_ratio = [section / self.head_dim for section in self.mrope_sections]
        vision_head_dim = self.vision_config.embed_dim // self.vision_config.num_heads
        scaled_sections = self._scale_mrope_sections_from_ratio(vision_head_dim, self._mrope_ratio)
        self.vision_config.mrope_sections = scaled_sections

        # Validate rope parameters (ignore M-ROPE specific keys since we use custom M-ROPE)
        if self.rope_parameters is not None and "type" in self.rope_parameters:
            self.rope_parameters["rope_type"] = self.rope_parameters["type"]
        rope_config_validation(self, ignore_keys={"mrope_sections"})

        # Layer types configuration
        self.layer_types = layer_types
        if self.layer_types is None:
            self.layer_types = [
                "sliding_attention"
                if self.sliding_window is not None and i >= self.max_window_layers
                else "full_attention"
                for i in range(self.num_hidden_layers)
            ]
        layer_type_validation(self.layer_types, self.num_hidden_layers)

        super().__init__(
            pad_token_id=pad_token_id,
            bos_token_id=bos_token_id,
            eos_token_id=eos_token_id,
            tie_word_embeddings=tie_word_embeddings,
            **kwargs,
        )

    @staticmethod
    def _scale_mrope_sections_from_ratio(head_dim: int, ratio: list[float]) -> list[int]:
        if head_dim <= 0:
            raise ValueError("head_dim must be positive when computing M-ROPE sections.")
        if len(ratio) != 3:
            raise ValueError("ratio must contain three entries for [temporal, height, width].")

        raw = [r * head_dim for r in ratio]
        base = [math.floor(val) for val in raw]
        remainders = [val - floor for val, floor in zip(raw, base)]

        # Guarantee a minimum allocation of 1 per dimension
        for i in range(len(base)):
            if base[i] < 1:
                base[i] = 1
                remainders[i] = 0.0

        total = sum(base)

        # Distribute remaining capacity based on largest remainders
        while total < head_dim:
            idx = max(range(len(base)), key=lambda i: remainders[i])
            base[idx] += 1
            remainders[idx] = 0.0
            total += 1

        # Trim excess while keeping each section >= 1
        while total > head_dim:
            candidates = [i for i in range(len(base)) if base[i] > 1]
            if not candidates:
                break
            idx = min(candidates, key=lambda i: remainders[i])
            base[idx] -= 1
            total -= 1

        return base


__all__ = ["ArlowConfig", "ArlowTextConfig", "ArlowVisionConfig"]
